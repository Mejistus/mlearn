{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的原理\n",
    "\n",
    "神经网络是一种模仿人脑神经元结构的计算模型，用于处理复杂的数据模式和关系。以下是神经网络的基本原理和构成要素。\n",
    "\n",
    "### 神经元\n",
    "\n",
    "神经网络的基本单位是神经元，神经元接收输入信号，通过加权求和和激活函数产生输出。一个神经元的数学表示为：\n",
    "\n",
    "$$\n",
    "y = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $x_i$ 是输入信号\n",
    "- $w_i$ 是权重\n",
    "- $b$ 是偏置\n",
    "- $f$ 是激活函数\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "激活函数用于引入非线性，常见的激活函数有：\n",
    "\n",
    "- Sigmoid 函数：\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- ReLU 函数：\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- Tanh 函数：\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "### 网络结构\n",
    "\n",
    "神经网络通常由多个层组成，包括输入层、隐藏层和输出层。每层由多个神经元组成。\n",
    "\n",
    "#### 输入层\n",
    "\n",
    "输入层接收外部输入数据，不进行任何计算，直接传递到下一层。\n",
    "\n",
    "#### 隐藏层\n",
    "\n",
    "隐藏层位于输入层和输出层之间，可以有一层或多层。每个隐藏层神经元接收前一层的输出，进行加权求和和激活。\n",
    "\n",
    "#### 输出层\n",
    "\n",
    "输出层产生最终的输出结果，其结构和神经元数量取决于具体任务。\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "前向传播是指从输入层开始，逐层计算每个神经元的输出，直到输出层。每层的输出作为下一层的输入。\n",
    "\n",
    "### 误差计算\n",
    "\n",
    "在训练过程中，通过损失函数计算预测输出与真实值之间的误差。常用的损失函数有均方误差和交叉熵损失。\n",
    "\n",
    "- 均方误差（MSE）：\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- 交叉熵损失：\n",
    "\n",
    "$$\n",
    "E = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### 反向传播\n",
    "\n",
    "反向传播用于更新神经网络的权重和偏置，以最小化损失函数。反向传播通过链式法则计算梯度，并使用优化算法（如梯度下降）进行参数更新。\n",
    "\n",
    "#### 梯度下降\n",
    "\n",
    "梯度下降通过以下公式更新权重和偏置：\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\eta \\frac{\\partial E}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial E}{\\partial w_{j,k}}=-(t_k-o_k)\\cdot \\sigma(\\sum_j{w_{j,k}\\cdot o_j})\\cdot (1 - \\sigma(\\sum_j{w_{j,k}\\cdot o_j}))\\cdot o_j\n",
    "$$\n",
    "\n",
    "其中 \\( \\eta \\) 是学习率。\n",
    "\n",
    "### 训练过程\n",
    "\n",
    "神经网络的训练过程包括以下步骤：\n",
    "\n",
    "1. 初始化权重和偏置。\n",
    "2. 进行前向传播计算输出。\n",
    "3. 计算损失函数值。\n",
    "4. 进行反向传播计算梯度。\n",
    "5. 更新权重和偏置。\n",
    "6. 重复以上步骤，直到损失函数收敛或达到预定的训练次数。\n",
    "\n",
    "通过这些步骤，神经网络能够逐渐优化其参数，提高对复杂模式和数据关系的建模能力。\n",
    "\n",
    "## 激活函数的选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 一室之不治，何以家国天下为？\n",
    "\n",
    "## 任务\n",
    "\n",
    "利用神经网络构建个分类动物的神经网络模型\n",
    "\n",
    "通过输入的 3 个属性（年龄，体重，体长），输出为动物类型（0,0,1）的单层神经网络\n",
    "\n",
    "## 网络设计\n",
    "\n",
    "输入层 隐藏层 输出层\n",
    "3 3 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class GenData:\n",
    "    def __init__(self):\n",
    "        self.animals = [\"dog\", \"cow\", \"monkey\"]\n",
    "\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        labels = []\n",
    "        data = [\n",
    "            # Dogs\n",
    "            [1.2, 18.7, 30.5],\n",
    "            [3.4, 45.2, 41.3],\n",
    "            [5.6, 70.1, 50.2],\n",
    "            [7.8, 95.5, 62.3],\n",
    "            [10.0, 120.3, 70.4],\n",
    "            [1.5, 22.1, 32.7],\n",
    "            [2.5, 35.0, 38.4],\n",
    "            [4.2, 55.6, 46.2],\n",
    "            [6.3, 82.0, 58.3],\n",
    "            [8.4, 105.5, 66.0],\n",
    "            [1.8, 25.0, 33.5],\n",
    "            [2.8, 38.4, 39.7],\n",
    "            [4.8, 60.2, 48.5],\n",
    "            [6.5, 85.0, 59.8],\n",
    "            [9.5, 115.0, 68.0],\n",
    "            [1.0, 15.0, 28.0],\n",
    "            [3.0, 40.0, 42.0],\n",
    "            [5.0, 65.0, 52.0],\n",
    "            [7.0, 90.0, 60.0],\n",
    "            [9.0, 115.0, 69.0],\n",
    "            # Cows\n",
    "            [1.2, 160.5, 110.5],\n",
    "            [3.4, 400.0, 140.0],\n",
    "            [5.6, 700.0, 160.0],\n",
    "            [7.8, 950.0, 180.0],\n",
    "            [10.0, 1200.0, 200.0],\n",
    "            [2.0, 220.0, 120.0],\n",
    "            [4.0, 480.0, 150.0],\n",
    "            [6.0, 750.0, 170.0],\n",
    "            [8.0, 1000.0, 190.0],\n",
    "            [12.0, 1350.0, 210.0],\n",
    "            [1.5, 180.0, 115.0],\n",
    "            [3.5, 420.0, 145.0],\n",
    "            [5.5, 680.0, 165.0],\n",
    "            [7.5, 930.0, 185.0],\n",
    "            [9.5, 1180.0, 205.0],\n",
    "            [2.5, 250.0, 125.0],\n",
    "            [4.5, 500.0, 155.0],\n",
    "            [6.5, 780.0, 175.0],\n",
    "            [8.5, 1050.0, 195.0],\n",
    "            [11.5, 1400.0, 215.0],\n",
    "            # Monkeys\n",
    "            [1.2, 18.7, 50.5],\n",
    "            [3.4, 30.2, 60.3],\n",
    "            [5.6, 45.1, 70.2],\n",
    "            [7.8, 60.5, 75.3],\n",
    "            [10.0, 75.3, 80.4],\n",
    "            [2.5, 22.1, 55.7],\n",
    "            [4.5, 35.0, 65.4],\n",
    "            [6.5, 50.6, 72.2],\n",
    "            [8.5, 65.0, 78.3],\n",
    "            [12.0, 80.0, 82.0],\n",
    "            [3.0, 25.0, 58.5],\n",
    "            [5.0, 38.4, 68.7],\n",
    "            [7.0, 55.2, 74.5],\n",
    "            [9.0, 70.0, 79.8],\n",
    "            [11.0, 85.0, 83.0],\n",
    "            [2.0, 20.0, 53.0],\n",
    "            [4.0, 32.0, 63.0],\n",
    "            [6.0, 48.0, 71.0],\n",
    "            [8.0, 62.0, 77.0],\n",
    "            [10.0, 77.0, 81.0],\n",
    "        ]\n",
    "        labels = [\n",
    "            # Dogs\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            [1, 0, 0],\n",
    "            # Cows\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 0],\n",
    "            # Monkeys\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class MyNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n",
    "        self.input = input_size\n",
    "        self.hidden = hidden_size\n",
    "        self.output = output_size\n",
    "        self.lr = learning_rate\n",
    "        self.Wi2h = [\n",
    "            [random.uniform(-1, 1) for _ in range(hidden_size)]\n",
    "            for _ in trange(input_size)\n",
    "        ]\n",
    "        self.Wh2o = [\n",
    "            [random.uniform(-1, 1) for _ in range(output_size)]\n",
    "            for _ in trange(hidden_size)\n",
    "        ]\n",
    "        self.Bi2h = [random.uniform(-1, 1) for _ in range(hidden_size)]\n",
    "        self.Bh2o = [random.uniform(-1, 1) for _ in range(output_size)]\n",
    "\n",
    "    def func(self, x):\n",
    "        if x < -25:\n",
    "            return 0.000001\n",
    "        if x > 10:\n",
    "            return 0.9999\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def Dfunc(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Input to hidden layer\n",
    "        Oi2h = [\n",
    "            sum(x * w for x, w in zip(inputs, col)) + b\n",
    "            for col, b in zip(zip(*self.Wi2h), self.Bi2h)\n",
    "        ]\n",
    "        Oh = list(map(self.func, Oi2h))\n",
    "\n",
    "        # Hidden to output layer\n",
    "        Oh2o = [\n",
    "            sum(x * w for x, w in zip(Oh, col)) + b\n",
    "            for col, b in zip(zip(*self.Wh2o), self.Bh2o)\n",
    "        ]\n",
    "        Oo = list(map(self.func, Oh2o))\n",
    "        return Oh, Oo\n",
    "\n",
    "    def backward(self, inputs, hidden_outputs, actual_outputs, expected_outputs):\n",
    "        # Output layer error and delta\n",
    "        Eout = [\n",
    "            expected - actual\n",
    "            for expected, actual in zip(expected_outputs, actual_outputs)\n",
    "        ]\n",
    "        Dout = [\n",
    "            error * self.Dfunc(output) for error, output in zip(Eout, actual_outputs)\n",
    "        ]\n",
    "\n",
    "        # Hidden layer error and delta\n",
    "        Ehide = [\n",
    "            sum(delta * w for delta, w in zip(Dout, col)) for col in zip(*self.Wh2o)\n",
    "        ]\n",
    "        Dhide = [\n",
    "            error * self.Dfunc(output) for error, output in zip(Ehide, hidden_outputs)\n",
    "        ]\n",
    "\n",
    "        # Update weights and biases from hidden to output layer\n",
    "        for i, hidden_output in enumerate(hidden_outputs):\n",
    "            for j, output_delta in enumerate(Dout):\n",
    "                self.Wh2o[i][j] += self.lr * output_delta * hidden_output\n",
    "        for j, output_delta in enumerate(Dout):\n",
    "            self.Bh2o[j] += self.lr * output_delta\n",
    "\n",
    "        # Update weights and biases from input to hidden layer\n",
    "        for i, input_val in enumerate(inputs):\n",
    "            for j, hidden_delta in enumerate(Dhide):\n",
    "                self.Wi2h[i][j] += self.lr * hidden_delta * input_val\n",
    "        for j, hidden_delta in enumerate(Dhide):\n",
    "            self.Bi2h[j] += self.lr * hidden_delta\n",
    "\n",
    "    def train(self, training_data, training_labels, epochs):\n",
    "        for epoch in trange(epochs):\n",
    "            for inputs, expected_outputs in zip(training_data, training_labels):\n",
    "                hidden_outputs, actual_outputs = self.forward(inputs)\n",
    "                self.backward(inputs, hidden_outputs, actual_outputs, expected_outputs)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        Oh, Oo = self.forward(inputs)\n",
    "        return Oo\n",
    "\n",
    "\n",
    "def test(test_data, test_label):\n",
    "    for i in range(len(test_data)):\n",
    "        test, label = test_data[i], test_label[i]\n",
    "        ans = nn.predict(test)\n",
    "        output = g.animals[ans.index(max(ans))]\n",
    "        print(\n",
    "            \"Predicted animal is\",\n",
    "            output,\n",
    "            \"should be\",\n",
    "            g.animals[label.index(1)],\n",
    "            label.index(1) == ans.index(max(ans)),\n",
    "        )\n",
    "\n",
    "\n",
    "def draw(data, labels):\n",
    "    # 将数据和标签分成三类\n",
    "    dogs = [data[i] for i in range(len(labels)) if labels[i] == [1, 0, 0]]\n",
    "    cows = [data[i] for i in range(len(labels)) if labels[i] == [0, 1, 0]]\n",
    "    monkeys = [data[i] for i in range(len(labels)) if labels[i] == [0, 0, 1]]\n",
    "    # Dogs\n",
    "    dogs_x = [d[0] for d in dogs]\n",
    "    dogs_y = [d[1] for d in dogs]\n",
    "    dogs_z = [d[2] for d in dogs]\n",
    "    # Cows\n",
    "    cows_x = [c[0] for c in cows]\n",
    "    cows_y = [c[1] for c in cows]\n",
    "    cows_z = [c[2] for c in cows]\n",
    "    # Monkeys\n",
    "    monkeys_x = [m[0] for m in monkeys]\n",
    "    monkeys_y = [m[1] for m in monkeys]\n",
    "    monkeys_z = [m[2] for m in monkeys]\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Dogs\n",
    "    ax.scatter(dogs_x, dogs_y, dogs_z, c=\"red\", label=\"Dogs\")\n",
    "    # Cows\n",
    "    ax.scatter(cows_x, cows_y, cows_z, c=\"green\", label=\"Cows\")\n",
    "    # Monkeys\n",
    "    ax.scatter(monkeys_x, monkeys_y, monkeys_z, c=\"blue\", label=\"Monkeys\")\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_zlabel(\"Feature 3\")\n",
    "    ax.set_title(\"3D Scatter Plot of Dogs, Cows, and Monkeys\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs = 50000\n",
    "import itertools\n",
    "\n",
    "g = GenData()\n",
    "data, labels = g.generate_data()\n",
    "draw(data, labels)\n",
    "data_set = list(zip(data, labels))\n",
    "shff = random.sample(data_set, len(data_set))\n",
    "data_set = shff\n",
    "train_set = data_set[:50]\n",
    "test_set = data_set[50:]\n",
    "\n",
    "data, labels = list(zip(*train_set))\n",
    "test_data, test_label = list(zip(*test_set))\n",
    "\n",
    "nn = MyNN(3, 5, 3, 0.01)\n",
    "nn.train(data, labels, epochs)\n",
    "test(test_data, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 6)\n",
    "        self.fc2 = nn.Linear(6, 6)\n",
    "        self.fc3 = nn.Linear(6, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 自定义函数实现二进制转十进制\n",
    "class BinaryToDecimal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        decimal = inputs.apply_(\n",
    "            lambda x: int(\"\".join(map(str, map(int, x.tolist()))), 2)\n",
    "        )\n",
    "        return decimal.float().view(-1, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "binary_to_decimal = BinaryToDecimal.apply\n",
    "\n",
    "# 创建神经网络实例\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def sigmoid(x: numbers.Number):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def liner(x: numbers.Number):\n",
    "        return x\n",
    "\n",
    "    def __init__(self, weight: float, bias: float, func=sigmoid, activation=0):\n",
    "        self.w = weight\n",
    "        self.b = bias\n",
    "        self.fw_connextions = []\n",
    "        self.bw_connextions = []\n",
    "        self.func = func\n",
    "        self.a = activation\n",
    "\n",
    "    def add_connextions(self, neuron):\n",
    "        self.fw_connextions.append(neuron)\n",
    "        self.bw_connextions.append(neuron)\n",
    "\n",
    "    def forward(self):\n",
    "        for conn in self.fw_connextions:\n",
    "            conn: Neuron\n",
    "            conn.a += self.w * self.a + self.b\n",
    "\n",
    "    def update(self):\n",
    "        self.a = self.func(self.a)\n",
    "\n",
    "    def backward(self):\n",
    "        for conn in self.bw_connextions:\n",
    "            conn: Neuron\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Neuron[{self.w:.2f},{self.b:2f},{self.a:.2f}]\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, *args: Neuron):\n",
    "        self.neurons = args\n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "\n",
    "    def connect(self, layer: Layer, full=True):  # type: ignore\n",
    "        if not full:\n",
    "            assert len(self.neurons) == len(layer.neurons)\n",
    "            for i, _ in enumerate(self.neurons):\n",
    "                self.neurons[i].add_connextions(layer.neurons[i])\n",
    "        else:\n",
    "            for i, _ in enumerate(self.neurons):\n",
    "                for j, _ in enumerate(layer.neurons):\n",
    "                    self.neurons[i].add_connextions(layer.neurons[j])\n",
    "        self.next_layer = layer\n",
    "        layer.prev_layer = self\n",
    "        return self\n",
    "\n",
    "    def forward(self):\n",
    "        for i, _ in enumerate(self.neurons):\n",
    "            self.neurons[i].forward()\n",
    "        for i, _ in enumerate(self.next_layer.neurons):\n",
    "            self.next_layer.neurons[i].update()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \",\".join(map(str, self.neurons))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \",\".join(map(str, self.neurons))\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        return\n",
    "\n",
    "    def fit(self, x: list[float], Y: list[float]):\n",
    "        return\n",
    "\n",
    "    def output():\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "i1, i2 = (\n",
    "    Neuron(1, 0, activation=1, func=Neuron.liner),\n",
    "    Neuron(1, 0, activation=0.5, func=Neuron.liner),\n",
    ")\n",
    "h11, h21 = (\n",
    "    Neuron(0.9, 0),\n",
    "    Neuron(0.3, 0),\n",
    ")\n",
    "\n",
    "o1, o2 = Neuron(0.2, 0), Neuron(0.8, 0)\n",
    "\n",
    "# input_layer = [i1, i2]\n",
    "# hidden_layer = [h11, h21]\n",
    "# output_layer = [o1, o2]\n",
    "input_layer = Layer(i1, i2)\n",
    "hidden_layer = Layer(h11, h21)\n",
    "output_layer = Layer(o1, o2)\n",
    "network = input_layer.connect(hidden_layer.connect(output_layer), full=False)\n",
    "\n",
    "print(input_layer)\n",
    "print(hidden_layer)\n",
    "input_layer.forward()\n",
    "print(hidden_layer)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # ? forward\n",
    "    for i, _ in enumerate(input_layer):\n",
    "        input_layer[i].forward()\n",
    "\n",
    "    for i in hidden_layer:\n",
    "        print(i, end=\" \")\n",
    "    print()\n",
    "    for i, _ in enumerate(hidden_layer):\n",
    "        hidden_layer[i].forward()\n",
    "\n",
    "    # for i in hidden_layer:\n",
    "    #     print(i, end=\" \")\n",
    "    for i in output_layer:\n",
    "        print(i, end=\" \")\n",
    "    # ?backward\n",
    "    return\n",
    "\n",
    "\n",
    "# train()\n",
    "\n",
    "# batches = 10000\n",
    "# for _ in trange(batches):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "i1, i2, i3 = (\n",
    "    Neuron(0, 0, activation=1),\n",
    "    Neuron(0, 0, activation=1),\n",
    "    Neuron(0, 0, activation=1),\n",
    ")\n",
    "h11, h21, n31 = Neuron(1, 1), Neuron(1, 1), Neuron(1, 1)\n",
    "n12 = Neuron(1, 1)\n",
    "o1 = Neuron(1, 1)\n",
    "input_layer = [i1, i2, i3]\n",
    "hidden_layer = [h11, h21, n31]\n",
    "output_layer = [o1]\n",
    "\n",
    "for i, neu in enumerate(input_layer):\n",
    "    input_layer[i].add_connextions(hidden_layer[i])\n",
    "\n",
    "for i, _ in enumerate(hidden_layer):\n",
    "    for j, __ in enumerate(output_layer):\n",
    "        hidden_layer[i].add_connextions(output_layer[j])\n",
    "\n",
    "\n",
    "def train():\n",
    "    # ? forward\n",
    "    for i, _ in enumerate(input_layer):\n",
    "        input_layer[i].forward()\n",
    "\n",
    "    for i in hidden_layer:\n",
    "        print(i, end=\" \")\n",
    "    print()\n",
    "    for i, _ in enumerate(hidden_layer):\n",
    "        hidden_layer[i].forward()\n",
    "    for i in output_layer:\n",
    "        print(i, end=\" \")\n",
    "    # ?backward\n",
    "    return\n",
    "\n",
    "\n",
    "train()\n",
    "\n",
    "batches = 10000\n",
    "for _ in trange(batches):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准 BP 和累积误差 BP\n",
    "\n",
    "累积 BP 算法与标准 BP 算法都很常用.一般来说,标准 BP 算法每次更新只针对单个样例,参数更新得非常频繁,而且对不同样例进行更新的效果可能出现“抵消”现象.因此,为了达到同样的累积误差极小点,标准 BP 算法往往需进行更多次数的迭代.累积 B P 算法直接针对累积误差最小化,它在读取整个训练集 D 一遍后才对参数进行更新,其参数更新的频率低得多.但在很多任务中,累积误差下降到一定程度之后,进一步下降会非常缓慢,这时标准 BP 往往会更快获得较好的解,尤其是在训练集 D 非常大时更明显.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# 定义神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 初始化网络、损失函数和优化器\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练网络\n",
    "for epoch in range(1):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00018487652414478362\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# 定义神经网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# 初始化网络、损失函数和优化器\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练网络\n",
    "for epoch in range(1):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
