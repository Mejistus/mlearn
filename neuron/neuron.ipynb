{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的原理\n",
    "\n",
    "神经网络是一种模仿人脑神经元结构的计算模型，用于处理复杂的数据模式和关系。以下是神经网络的基本原理和构成要素。\n",
    "\n",
    "### 神经元\n",
    "\n",
    "神经网络的基本单位是神经元，神经元接收输入信号，通过加权求和和激活函数产生输出。一个神经元的数学表示为：\n",
    "\n",
    "$$\n",
    "y = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\( x_i \\) 是输入信号\n",
    "- \\( w_i \\) 是权重\n",
    "- \\( b \\) 是偏置\n",
    "- \\( f \\) 是激活函数\n",
    "\n",
    "### 激活函数\n",
    "\n",
    "激活函数用于引入非线性，常见的激活函数有：\n",
    "\n",
    "- Sigmoid 函数：\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- ReLU 函数：\n",
    "\n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- Tanh 函数：\n",
    "\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "### 网络结构\n",
    "\n",
    "神经网络通常由多个层组成，包括输入层、隐藏层和输出层。每层由多个神经元组成。\n",
    "\n",
    "#### 输入层\n",
    "\n",
    "输入层接收外部输入数据，不进行任何计算，直接传递到下一层。\n",
    "\n",
    "#### 隐藏层\n",
    "\n",
    "隐藏层位于输入层和输出层之间，可以有一层或多层。每个隐藏层神经元接收前一层的输出，进行加权求和和激活。\n",
    "\n",
    "#### 输出层\n",
    "\n",
    "输出层产生最终的输出结果，其结构和神经元数量取决于具体任务。\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "前向传播是指从输入层开始，逐层计算每个神经元的输出，直到输出层。每层的输出作为下一层的输入。\n",
    "\n",
    "### 误差计算\n",
    "\n",
    "在训练过程中，通过损失函数计算预测输出与真实值之间的误差。常用的损失函数有均方误差和交叉熵损失。\n",
    "\n",
    "- 均方误差（MSE）：\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- 交叉熵损失：\n",
    "\n",
    "$$\n",
    "E = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "### 反向传播\n",
    "\n",
    "反向传播用于更新神经网络的权重和偏置，以最小化损失函数。反向传播通过链式法则计算梯度，并使用优化算法（如梯度下降）进行参数更新。\n",
    "\n",
    "#### 梯度下降\n",
    "\n",
    "梯度下降通过以下公式更新权重和偏置：\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b - \\eta \\frac{\\partial E}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial E}{\\partial w_{j,k}}=-(t_k-o_k)\\cdot \\sigma(\\sum_j{w_{j,k}\\cdot o_j})\\cdot (1 - \\sigma(\\sum_j{w_{j,k}\\cdot o_j}))\\cdot o_j\n",
    "$$\n",
    "\n",
    "其中 \\( \\eta \\) 是学习率。\n",
    "\n",
    "### 训练过程\n",
    "\n",
    "神经网络的训练过程包括以下步骤：\n",
    "\n",
    "1. 初始化权重和偏置。\n",
    "2. 进行前向传播计算输出。\n",
    "3. 计算损失函数值。\n",
    "4. 进行反向传播计算梯度。\n",
    "5. 更新权重和偏置。\n",
    "6. 重复以上步骤，直到损失函数收敛或达到预定的训练次数。\n",
    "\n",
    "通过这些步骤，神经网络能够逐渐优化其参数，提高对复杂模式和数据关系的建模能力。\n",
    "\n",
    "## 激活函数的选择\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 一室之不治，何以家国天下为？\n",
    "\n",
    "## 任务\n",
    "\n",
    "从最简单的样例开始利用神经网络构建个转换二进制的神经网络模型\n",
    "\n",
    "通过输入的 3 个 bit 位，构建一个能够将 3bit 二进制数据转换成 10 进制数据的单层神经网络\n",
    "\n",
    "## 输出\n",
    "\n",
    "0 ～ 7\n",
    "\n",
    "## 输入\n",
    "\n",
    "000 ～ 111\n",
    "\n",
    "## 网络设计\n",
    "\n",
    "输入层 隐藏层 输出层\n",
    "3 2 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# 定义神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 6)\n",
    "        self.fc2 = nn.Linear(6, 6)\n",
    "        self.fc3 = nn.Linear(6, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 自定义函数实现二进制转十进制\n",
    "class BinaryToDecimal(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs):\n",
    "        ctx.save_for_backward(inputs)\n",
    "        decimal = inputs.apply_(\n",
    "            lambda x: int(\"\".join(map(str, map(int, x.tolist()))), 2)\n",
    "        )\n",
    "        return decimal.float().view(-1, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "binary_to_decimal = BinaryToDecimal.apply\n",
    "\n",
    "# 创建神经网络实例\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numbers\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def sigmoid(x: numbers.Number):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def liner(x: numbers.Number):\n",
    "        return x\n",
    "\n",
    "    def __init__(self, weight: float, bias: float, func=sigmoid, activation=0):\n",
    "        self.w = weight\n",
    "        self.b = bias\n",
    "        self.fw_connextions = []\n",
    "        self.bw_connextions = []\n",
    "        self.func = func\n",
    "        self.a = activation\n",
    "\n",
    "    def add_connextions(self, neuron):\n",
    "        self.fw_connextions.append(neuron)\n",
    "        self.bw_connextions.append(neuron)\n",
    "\n",
    "    def forward(self):\n",
    "        for conn in self.fw_connextions:\n",
    "            conn: Neuron\n",
    "            conn.a += self.w * self.a + self.b\n",
    "\n",
    "    def update(self):\n",
    "        self.a = self.func(self.a)\n",
    "\n",
    "    def backward(self):\n",
    "        for conn in self.bw_connextions:\n",
    "            conn: Neuron\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Neuron[{self.w:.2f},{self.b:2f},{self.a:.2f}]\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, *args: Neuron):\n",
    "        self.neurons = args\n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "\n",
    "    def connect(self, layer: Layer, full=True):  # type: ignore\n",
    "        if not full:\n",
    "            assert len(self.neurons) == len(layer.neurons)\n",
    "            for i, _ in enumerate(self.neurons):\n",
    "                self.neurons[i].add_connextions(layer.neurons[i])\n",
    "        else:\n",
    "            for i, _ in enumerate(self.neurons):\n",
    "                for j, _ in enumerate(layer.neurons):\n",
    "                    self.neurons[i].add_connextions(layer.neurons[j])\n",
    "        self.next_layer = layer\n",
    "        layer.prev_layer = self\n",
    "        return self\n",
    "\n",
    "    def forward(self):\n",
    "        for i, _ in enumerate(self.neurons):\n",
    "            self.neurons[i].forward()\n",
    "        for i, _ in enumerate(self.next_layer.neurons):\n",
    "            self.next_layer.neurons[i].update()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \",\".join(map(str, self.neurons))\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \",\".join(map(str, self.neurons))\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        return\n",
    "\n",
    "    def fit(self, x: list[float], Y: list[float]):\n",
    "        return\n",
    "\n",
    "    def output():\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron[1.00,0.000000,1.00],Neuron[1.00,0.000000,0.50]\n",
      "Neuron[0.90,0.000000,0.00],Neuron[0.30,0.000000,0.00]\n",
      "Neuron[0.90,0.000000,0.73],Neuron[0.30,0.000000,0.62]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "i1, i2 = (\n",
    "    Neuron(1, 0, activation=1, func=Neuron.liner),\n",
    "    Neuron(1, 0, activation=0.5, func=Neuron.liner),\n",
    ")\n",
    "h11, h21 = (\n",
    "    Neuron(0.9, 0),\n",
    "    Neuron(0.3, 0),\n",
    ")\n",
    "\n",
    "o1, o2 = Neuron(0.2, 0), Neuron(0.8, 0)\n",
    "\n",
    "# input_layer = [i1, i2]\n",
    "# hidden_layer = [h11, h21]\n",
    "# output_layer = [o1, o2]\n",
    "input_layer = Layer(i1, i2)\n",
    "hidden_layer = Layer(h11, h21)\n",
    "output_layer = Layer(o1, o2)\n",
    "network = input_layer.connect(hidden_layer.connect(output_layer), full=False)\n",
    "\n",
    "print(input_layer)\n",
    "print(hidden_layer)\n",
    "input_layer.forward()\n",
    "print(hidden_layer)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # ? forward\n",
    "    for i, _ in enumerate(input_layer):\n",
    "        input_layer[i].forward()\n",
    "\n",
    "    for i in hidden_layer:\n",
    "        print(i, end=\" \")\n",
    "    print()\n",
    "    for i, _ in enumerate(hidden_layer):\n",
    "        hidden_layer[i].forward()\n",
    "\n",
    "    # for i in hidden_layer:\n",
    "    #     print(i, end=\" \")\n",
    "    for i in output_layer:\n",
    "        print(i, end=\" \")\n",
    "    # ?backward\n",
    "    return\n",
    "\n",
    "\n",
    "# train()\n",
    "\n",
    "# batches = 10000\n",
    "# for _ in trange(batches):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3) <class 'tuple'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "i1, i2, i3 = (\n",
    "    Neuron(0, 0, activation=1),\n",
    "    Neuron(0, 0, activation=1),\n",
    "    Neuron(0, 0, activation=1),\n",
    ")\n",
    "h11, h21, n31 = Neuron(1, 1), Neuron(1, 1), Neuron(1, 1)\n",
    "n12 = Neuron(1, 1)\n",
    "o1 = Neuron(1, 1)\n",
    "input_layer = [i1, i2, i3]\n",
    "hidden_layer = [h11, h21, n31]\n",
    "output_layer = [o1]\n",
    "\n",
    "for i, neu in enumerate(input_layer):\n",
    "    input_layer[i].add_connextions(hidden_layer[i])\n",
    "\n",
    "for i, _ in enumerate(hidden_layer):\n",
    "    for j, __ in enumerate(output_layer):\n",
    "        hidden_layer[i].add_connextions(output_layer[j])\n",
    "\n",
    "\n",
    "def train():\n",
    "    # ? forward\n",
    "    for i, _ in enumerate(input_layer):\n",
    "        input_layer[i].forward()\n",
    "\n",
    "    for i in hidden_layer:\n",
    "        print(i, end=\" \")\n",
    "    print()\n",
    "    for i, _ in enumerate(hidden_layer):\n",
    "        hidden_layer[i].forward()\n",
    "    for i in output_layer:\n",
    "        print(i, end=\" \")\n",
    "    # ?backward\n",
    "    return\n",
    "\n",
    "\n",
    "train()\n",
    "\n",
    "batches = 10000\n",
    "for _ in trange(batches):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
