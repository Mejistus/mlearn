# 决策树

## 1. 原理

## 2.最优划分推导

### 信息熵

设集合 D 中的事件$a_1, a_2...a_k$发生概率依次为$p_1, p_2...p_k$

则 D 的信息熵定义为 $$Ent(D) = -\sum_{k=1}^{|y|}p_k\cdot log_2{p_k} $$
​ 如果以$a_i$作为节点对原事件集合进行划分，会得到多个新的子集$D_1,D_2...D_m$

那么新产生的总信息增益会发生改变，即：$Ent(D)=\sum_{i=1}^{m}Ent(D_i)\cdot \lambda_i$

其中$\lambda_i$表示子集所占有的权重，具体而言就是$\frac{|D_i|}{|D|}$。

> [!IMPORTANT]
>
> 为了提高决策树的效率，需要优先选择分解后能够导致信息熵更低的节点。
>
> 至于为什么需要权重，是由于新产生的子集有可能是很小的，但是$Ent$并不能考虑到这一点，在具有相当水平的情况下，当然应该优先选择数量最多的属性。

## 3.实现
